{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "from nltk import ngrams\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "# this function returns a list of tokenized and stemmed words of any text\n",
    "def get_tokenized_list(doc_text):\n",
    "    tokens = nltk.word_tokenize(doc_text)\n",
    "    return tokens\n",
    "\n",
    "# This function will performing stemming on tokenized words\n",
    "def word_stemmer(token_list):\n",
    "  ps = nltk.stem.PorterStemmer()\n",
    "  stemmed = []\n",
    "  for words in token_list:\n",
    "    stemmed.append(ps.stem(words))\n",
    "  return stemmed\n",
    "\n",
    "# Function to remove stopwords from tokenized word list\n",
    "def remove_stopwords(doc_text):\n",
    "  cleaned_text = []\n",
    "  for words in doc_text:\n",
    "    if words not in stop_words:\n",
    "      cleaned_text.append(words)\n",
    "  return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "        'Home team will win the match because away team has a lot of injured players',\n",
    "        'Away team is in good health!',\n",
    "        'Both teams are in a bad situation. They both lost last three matches',\n",
    "        'Home team is in good health!',\n",
    "        'Home team won last four matches while away team lost last two matches',\n",
    "        'We expected home team and away team will make at least one goal'\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpu = pd.read_csv('completati/description_predictions.csv', index_col=0)\n",
    "\n",
    "with open(\"sinonimi.json\") as jsonFile:\n",
    "    jsonObject = json.load(jsonFile)\n",
    "    jsonFile.close()\n",
    "\n",
    "corpus = []\n",
    "for cor, c in corpu.iterrows():\n",
    "    h_team, a_team, description = c.h_team[0], c.a_team[0], c.description[0]\n",
    "    \n",
    "    syn = {}\n",
    "    for key in jsonObject.keys():\n",
    "        if (h_team in key) or (key in h_team):\n",
    "            syn['home team'] = jsonObject[key] \n",
    "            \n",
    "        if (a_team in key) or (key in a_team):\n",
    "            syn['away team'] = jsonObject[key] \n",
    "    \n",
    "    description = description.lower()\n",
    "    for key in syn.keys():\n",
    "        for val in syn[key]:\n",
    "            description = description.replace(val.lower(), key)\n",
    "\n",
    "    corpus.append(description)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_corpus = []\n",
    "for doc in corpus:\n",
    "  tokens = get_tokenized_list(doc)\n",
    "  doc_text = remove_stopwords(tokens)\n",
    "  doc_text  = word_stemmer(doc_text)\n",
    "  #doc_text  = word_stemmer(tokens)\n",
    "  doc_text = ' '.join(doc_text)\n",
    "  cleaned_corpus.append(doc_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizerX = TfidfVectorizer()\n",
    "vectorizerX.fit(cleaned_corpus) #impara il vocabolario e idf dal training set\n",
    "doc_vector = vectorizerX.transform(cleaned_corpus) #trasforma i documenti in una matrice document-term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(doc_vector.toarray(), columns=vectorizerX.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=3, random_state=0).fit(doc_vector[:200])\n",
    "#print(kmeans.labels_)\n",
    "#print(kmeans.predict(doc_vector[200:]))\n",
    "lett = []\n",
    "for km in kmeans.labels_:\n",
    "    if km == 0:\n",
    "        lett.append('P')\n",
    "    if km == 1:\n",
    "        lett.append('V')\n",
    "    if km == 2:\n",
    "        lett.append('N')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b51fba03621ea2707aa24f127a4f74542be1adffaa0a8f5a15c5b0606de1417a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
