{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import os\n",
    "import pathlib\n",
    "from tqdm import tqdm\n",
    "from queue import Empty\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quello che viene fatto è fare scraping attraverso https://fbref.com/it/comp/11/2021-2022/Statistiche-di-Serie-A-2021-2022 nella stagione 2021-2022 e vengono estratte tutte le informazioni relative alle varie squadre che giocano nella stagione. \n",
    "\n",
    "I dati provenienti da diversi link vengono mergiati. \n",
    "Per poter classificare ho pensato a 3 classi: V (vittoria), P (persa), N (pareggio).\n",
    "Il problema è che scaricando tutte le partite per tutte le squadre, ci saranno partite doppie, ad esempio: \n",
    "- Atalanta V Bologna\n",
    "- Bologna P Atalanta\n",
    "\n",
    "Per questo vado a considerare nella classificazione solo le squadre in casa, così quando si vuole predire il risultato di una partita, ottengo la classificazione per la squadra di casa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Serie A matches\n",
    "Per la realizzazione del dataset ho trovato in letteratura l'elenco dei dati più importanti per la predizione dei risultati delle partite. \n",
    "\n",
    "https://iopscience.iop.org/article/10.1088/1757-899X/226/1/012099/pdf\n",
    "\n",
    "Per questo dataset ho utilizzato le statistiche della stagione 2021/22"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'User-Agent': \n",
    "                'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/47.0.2526.106 Safari/537.36'}\n",
    "\n",
    "page = \"https://fbref.com/it/comp/11/2021-2022/Statistiche-di-Serie-A-2021-2022\"\n",
    "pageTree = requests.get(page)\n",
    "soup = BeautifulSoup(pageTree.text, features=\"lxml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = soup.select('table.stats_table')[0]\n",
    "a_hrefs = table.find_all('a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il dizionario ha come chiave il campo estratto dalle colonne nelle tabelle dei vari link, mentre come valore la stringa con la quale deve essere sostituito."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_fields = {\n",
    "    'Squadra': 'team',\n",
    "    'Avversario': 'avv',\n",
    "    'Rf': 'goals',\n",
    "    'Rs': 'gs',\n",
    "    'Tiri': 'total_shots',\n",
    "    'Tiri.1': 'shots_on_target',\n",
    "    'Rigori': 'goals_on_penalty',\n",
    "    'Rig T': 'total_penalties',\n",
    "    'Compl': 'completed_passings',\n",
    "    'Tent': 'total_passings',\n",
    "    'Angoli': 'corners',\n",
    "    'Poss.': 'percentage_possession',\n",
    "    'Falli': 'fouls',\n",
    "    'Amm.': 'yellow_cards',\n",
    "    'Esp.': 'red_cards',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams = [a_href for a_href in a_hrefs if '/squadre/' in str(a_href)]        \n",
    "#for team in teams:\n",
    "teams.sort(key=lambda x: x.contents[0]) #ordinamento per titolo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estrazione dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In questa sezione vado ad estrarre il dataset completo di tutte le partite giocate per ogni squadra\n",
    "\n",
    "Il dizionario sottostante contiene come chiave i link ai bottoni che permettono di portare ai link dei vari dataset della squadra e come valore il nome della parte di html che permette di accedere al dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "util_hrefs = {\n",
    "    'all_comps/shooting/': {\n",
    "        'section': 'Tiri',\n",
    "        'columns': ['Data', 'Tiri','Tiri.1','Rigori','Rig T']\n",
    "        },\n",
    "    'all_comps/passing/': {\n",
    "        'section': 'Passaggi',\n",
    "        'columns': ['Data', 'Compl.', 'Tent,']\n",
    "        },\n",
    "        \n",
    "    'all_comps/passing_types': {\n",
    "        'section': 'Tipologie di passaggi',\n",
    "        'columns': ['Data', 'Angoli']\n",
    "        },\n",
    "    'all_comps/possession': {\n",
    "        'section': 'Possesso palla',\n",
    "        'columns': ['Data', 'Poss.']\n",
    "        },\n",
    "    'all_comps/misc': {\n",
    "        'section': 'Statistiche varie',\n",
    "        'columns': ['Data', 'Falli', 'Amm.', 'Esp.']\n",
    "        },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = pd.DataFrame()\n",
    "home_games = pd.DataFrame()\n",
    "away_games = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [00:47<00:00,  2.35s/it]\n"
     ]
    }
   ],
   "source": [
    "for team in tqdm(teams):\n",
    "    team_name = team.contents[0]\n",
    "    link = f\"https://fbref.com{team.get('href')}\"\n",
    "    data = requests.get(link)\n",
    "    games = pd.read_html(data.text, match=\"Punteggi e partite\")[0]\n",
    "\n",
    "    matches = games[games['Competizione'] == 'Serie A']\n",
    "    matches = matches[['Data', 'Stadio', 'Risultato', 'Rf','Avversario','Rs']]\n",
    "    #match[home_main_factors[0]] = team_name\n",
    "    matches.insert(0, \"Squadra\", team_name)\n",
    "    #matches = matches.rename(columns={'Rf': home_main_factors[1], 'Avversario': away_main_factors[0], 'Rs': away_main_factors[1]})\n",
    "    \n",
    "    #ottengo il dataset della singola squadra con tutti i dati che servono\n",
    "    soup = BeautifulSoup(data.text, features=\"lxml\")\n",
    "    links = soup.find_all('a')\n",
    "    href_links = [l.get(\"href\") for l in links]\n",
    "\n",
    "    old_dataset = matches\n",
    "    for href_key, section_value in util_hrefs.items():\n",
    "        div_links = [l for l in href_links if l and href_key in l]\n",
    "        #prendo il link e ottengo l'html\n",
    "        html = requests.get(f\"https://fbref.com{div_links[0]}\")\n",
    "        \n",
    "        section, columns = section_value.items()\n",
    "        section, columns = section[1], columns[1]\n",
    "        #ottengo il dataset della pima sezione indicata nel match\n",
    "        dataset_section = pd.read_html(html.text, match=section)[0]\n",
    "        #elimino l'intestazione \"Di: NomeSquadra\"\n",
    "        dataset_section.columns = dataset_section.columns.droplevel()\n",
    "        #filtro il dataset per la competizione\n",
    "        dataset_section = dataset_section[dataset_section['Competizione'] == 'Serie A']\n",
    "        #filtro il dataset per le colonne selezionate\n",
    "        dataset_section = dataset_section[[col for col in columns]]\n",
    "        #print(dataset_section.columns)\n",
    "        if section == 'Passaggi':\n",
    "            column_names = dataset_section.columns.values\n",
    "            column_names[1] = 'Compl'\n",
    "            column_names[5] = 'Tent'\n",
    "            dataset_section = dataset_section[['Data','Compl','Tent']]\n",
    "\n",
    "        old_dataset = pd.merge(old_dataset, dataset_section, on = [\"Data\"])\n",
    "        \n",
    "    datasets = datasets.append(old_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creo i due dataset, quello con le partite di casa e quello con quelle in trasferta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "760 380 380\n"
     ]
    }
   ],
   "source": [
    "home_games = datasets[datasets['Stadio']=='Casa']\n",
    "away_games = datasets[datasets['Stadio']=='Ospiti']\n",
    "\n",
    "print(len(datasets), len(home_games), len(away_games))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rename_home_fields = {key: 'h_'+value for key, value in rename_fields.items()}\n",
    "rename_home = rename_home_fields\n",
    "rename_home['Rs'] = 'a_goals'\n",
    "rename_home['Avversario'] = 'a_team'\n",
    "\n",
    "rename_away_fields = {key: 'a_'+value for key, value in rename_fields.items()}\n",
    "rename_away = rename_away_fields\n",
    "rename_away['Rs'] = 'h_goals'\n",
    "rename_away['Avversario'] = 'h_team'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "home_games = home_games.rename(columns=rename_home)\n",
    "away_games = away_games.rename(columns=rename_away)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_matches = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Controllo dimensioni dataset, che dovrebbero essere 20*16 partite ciascuno, quindi 380"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "380 380\n"
     ]
    }
   ],
   "source": [
    "print(len(home_games), len(away_games))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match delle stesse partite\n",
    "Il dataset iniziale contiene tutte le partite da tutte le squadre, quindi si passa da un dataset di n elementi ad un dataset di n/2.\n",
    "Matcho ogni partita del dataset home con la partita corrispondente del dataset away"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "away_columns_needed = [\"a_team\", \"a_total_shots\", \"a_shots_on_target\", \"a_goals_on_penalty\", \"a_total_penalties\", \"a_corners\", \"a_yellow_cards\", \"a_red_cards\", \"a_fouls\", \"a_completed_passings\",  \"a_total_passings\", \"a_percentage_possession\", \"Data\", \"h_team\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, home_match in home_games.iterrows():\n",
    "    away_match = away_games[(away_games['Data'] == home_match['Data']) & (away_games['h_team'] == home_match['h_team']) & (away_games['a_team'] == home_match['a_team'])]\n",
    "    away_match_reduced = away_match[away_columns_needed]\n",
    "    home_match = pd.merge(home_match.to_frame().T, away_match_reduced, on = [\"Data\", \"h_team\", \"a_team\"])\n",
    "    normalized_matches = normalized_matches.append(home_match)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ordiniamo le partite secondo la data in cui vengono giocate, perché se il modello voglio allenarlo con le ultime 5 partite, mi basta prendere le 5 più recenti in base all'ordine in cui sono salvate nel file csv. Una volta ordinato il dataset per Data della partita, elimino le colonne Stadio e Data tanto non mi servono per la classificazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_matches['Data'] = pd.to_datetime(normalized_matches['Data'], format='%d-%m-%Y')\n",
    "normalized_matches = normalized_matches.sort_values(by=['Data'], ascending=True)\n",
    "normalized_matches = normalized_matches.drop(columns=['Stadio', \"Data\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una volta definito l'ordine di appariszione delle colonne, salvo il dataframe secondo questo ordine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_needed = ['h_team', 'h_goals', 'a_team', 'a_goals', 'Risultato', \n",
    "        'h_total_shots', 'h_shots_on_target', 'a_total_shots', 'a_shots_on_target', \n",
    "        'h_goals_on_penalty', 'h_total_penalties', 'a_goals_on_penalty', 'a_total_penalties', \n",
    "        'h_completed_passings', 'h_total_passings', 'a_completed_passings', 'a_total_passings',\n",
    "        'h_corners', 'a_corners', \n",
    "        'h_fouls', 'h_yellow_cards', 'h_red_cards', 'a_yellow_cards', 'a_red_cards', 'a_fouls', \n",
    "        'h_percentage_possession', 'a_percentage_possession']\n",
    "        \n",
    "normalized_matches = normalized_matches[columns_needed]\n",
    "normalized_matches.to_csv(\"normalized_matches.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b51fba03621ea2707aa24f127a4f74542be1adffaa0a8f5a15c5b0606de1417a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
