{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ottenimento link news\n",
    "\n",
    "Questo script si pone come obiettivo quello di recuperare brevi descrizioni dal sito https://footballpredictions.com/ pubblicati prima di ciascun match.\n",
    "Andiamo a recuperare i dati relativi ad ogni partita andando a cercare nel link il giorno relativo alla partita. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = 'https://footballpredictions.com/footballpredictions/?date='\n",
    "format_date = '%d-%m-%Y'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Idee da implementare.\n",
    "Raggruppamento per data: \n",
    "* raggruppare le partite per la data, in modo tale da cercare una sola volta il link relativo alle partite che si svolgono in quella data e non comporre tanti link quante le partite\n",
    "* la seconda è recuperare tutte le date e poi nell'array fare un select distinct (remove duplicates)\n",
    "\n",
    "Una volta ottenuti i link andare a cercare nella pagina tutti i link sotto al div relativo alla Serie A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches = pd.read_csv('matches.csv', index_col=0)[['h_team', 'a_team', 'date']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_dates = matches[['date']]\n",
    "\n",
    "urls = {}\n",
    "for i, date in match_dates.iterrows():\n",
    "    page = 'https://footballpredictions.com/footballpredictions/?date='\n",
    "    string_data=date.iloc[0]\n",
    "    converted_data = pd.to_datetime(string_data, format='%Y-%m-%d', errors='coerce').date()\n",
    "    link_data = converted_data.strftime(format_date)\n",
    "    page += link_data\n",
    "    urls[str(converted_data)] = page"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ricerca sulla pagina footballpredictions.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "headers = {'User-Agent': \n",
    "                'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/47.0.2526.106 Safari/537.36'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ho notato che i link relativi alle news contengono la parola serieapredictions, quindi ottengo tutti i link e poi filtro per quelli contenente la parola"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quello che vado a fare è creare un dizionario dove come chiave ho la data del match, mentre come valore ho un array di link che portano a pagine contenenti le news e predizioni delle squadre coinvolte nel match. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 119/119 [05:22<00:00,  2.71s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "links_of_pages_by_date = {}\n",
    "for key, link_page in tqdm(urls.items()):\n",
    "    pageTree = requests.get(link_page)\n",
    "    soup = BeautifulSoup(pageTree.text, features=\"lxml\")\n",
    "    links = soup.find_all('a', href=True)\n",
    "    links = [link['href'] for link in links if 'serieapredictions' in link['href']]\n",
    "    cleaned_links = list(dict.fromkeys(links))\n",
    "    cleaned_links.pop(0)\n",
    "    links_of_pages_by_date[key] = cleaned_links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Salvataggio del dizionario su file json. In questo modo la prossima volta lo leggo senza andare a fare scraping per riottenere i link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"keyDat_valueLinks.json\", \"w\") as fp:\n",
    "    json.dump(links_of_pages_by_date,fp, indent=4) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b51fba03621ea2707aa24f127a4f74542be1adffaa0a8f5a15c5b0606de1417a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
